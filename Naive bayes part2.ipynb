{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f37781a-5c4f-4536-9e2d-3ad19b69038d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. A company conducted a survey of its employees and found that 70% of the employees use the\n",
    "company's health insurance plan, while 40% of the employees who use the plan are smokers. What is the\n",
    "probability that an employee is a smoker given that he/she uses the health insurance plan?\n",
    "\n",
    "We can solve this problem using Bayes' theorem. \n",
    "\n",
    "Lets \n",
    "   A - Employee uses the company's health insurance plan.\n",
    "   B - Employee is a smoker.\n",
    "P(A) = 0.70 (probability that an employee uses the health insurance plan)\n",
    "P(B|A) = 0.40 (probability that an employee is a smoker given that they use the health insurance plan)\n",
    "\n",
    "We want to find P(B|A), the probability that an employee is a smoker given that they use the health insurance plan.\n",
    "\n",
    "By Bayes' theorem, we have:\n",
    "     P(B/A) = (P(B)*P(A/B))/P(A)\n",
    "where:\n",
    "  P(B∣A) is the probability that an employee is a smoker given that they use the health insurance plan (what we want to find).\n",
    "  P(A∣B) is the probability that an employee uses the health insurance plan given that they are a smoker. Since we don't have this information directly, we'll need to use the formula and the provided information to find it.\n",
    "  P(B) is the probability that an employee is a smoker, which we don't have directly, but it can be calculated using the law of total probability\n",
    "  P(A) is the probability that an employee uses the health insurance plan (given as 0.70).\n",
    "To find P(B), we use the law of total probability:\n",
    "    P(B) = (P(B/A)*P(A))+(P(B/~A)*P(~A) = (0.4*0.7)+(0.6*0.3) = 0.46\n",
    "    P(A/B) = (P(B/A)*P(A))/P(B) = (0.40*0.70)/0.46 = o.6\n",
    "To fint the probability that an employee is a smoker given that they use the health insurance plan,\n",
    "                P(B/A) = (P(B)*P(A/B))/P(A) = (0.46*0.6)/0.7 = 0.26\n",
    " probability that an employee is a smoker given that they use the health insurance plan = 0.26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b9e07b-e7be-43e8-82d5-9d71b54f1a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the difference between Bernoulli Naive Bayes and Multinomial Naive Bayes?\n",
    "\n",
    "Bernoulli Naive Bayes:\n",
    "    Bernoulli Naive Bayes is designed for binary or binarized features. It assumes that the features are binary and takes values of 0 or 1.\n",
    "    It is commonly used in text classification tasks, where the presence or absence of words in a document is considered, without considering their frequency.\n",
    "    This model is suitable when the presence or absence of a particular feature is more important than its frequency.\n",
    "Multinomial Naive Bayes:\n",
    "    Multinomial Naive Bayes is designed for features that represent counts or frequencies. It assumes that features have discrete frequency counts.\n",
    "    It is commonly used for text classification tasks, where the frequency of terms or words in a document is considered as a feature.\n",
    "    This model is suitable when the frequency of occurrence of each feature is important for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff14cdbe-1645-4ca3-ba60-54c86b4f10b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How does Bernoulli Naive Bayes handle missing values?\n",
    "\n",
    "In the context of Bernoulli Naive Bayes, the handling of missing values can depend on the specific implementation or library used. However, in general, the typical approach is to treat missing values as a separate category or class. When a feature is missing for a particular data point, it is often treated as if that feature is not present or has a value of 0.\n",
    "\n",
    "For example, in the case of text classification, if a certain word is missing from a document, it may be treated as if the word is not present in the document and is assigned a value of 0. This is in line with the assumption of Bernoulli Naive Bayes that features are binary, and the absence or presence of a feature is considered for classification.\n",
    "\n",
    "When using Bernoulli Naive Bayes in practice, it's important to preprocess the data and handle missing values appropriately to ensure that the model can effectively learn from the available data. Some common techniques for handling missing values in the context of Bernoulli Naive Bayes include imputation, where missing values are replaced with estimated values based on the available data, or treating missing values as a separate category, as mentioned earlier.\n",
    "\n",
    "Overall, the handling of missing values in Bernoulli Naive Bayes involves treating the missing values as if the corresponding features are not present or have a value of 0, aligning with the binary nature of the features in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa03258-b74a-48cc-8019-fec61c6c02b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. Can Gaussian Naive Bayes be used for multi-class classification?\n",
    "\n",
    "yes,Gaussian NAive Bayes be used for multi-class classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1092ebf4-e27f-41ff-b3e7-6243e62e1d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Assignment:\n",
    "Data preparation:\n",
    "Download the \"Spambase Data Set\" from the UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/\n",
    "datasets/Spambase). This dataset contains email messages, where the goal is to predict whether a message\n",
    "is spam or not based on several input features.\n",
    "Implementation:\n",
    "Implement Bernoulli Naive Bayes, Multinomial Naive Bayes, and Gaussian Naive Bayes classifiers using the\n",
    "scikit-learn library in Python. Use 10-fold cross-validation to evaluate the performance of each classifier on the\n",
    "dataset. You should use the default hyperparameters for each classifier.\n",
    "Results:\n",
    "Report the following performance metrics for each classifier:\n",
    "Accuracy\n",
    "Precision\n",
    "Recall\n",
    "F1 score\n",
    "Discussion:\n",
    "Discuss the results you obtained. Which variant of Naive Bayes performed the best? Why do you think that is\n",
    "the case? Are there any limitations of Naive Bayes that you observed?\n",
    "Conclusion:\n",
    "Summarise your findings and provide some suggestions for future work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8dc024c9-6d36-4211-9976-a2254d593bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b242e265-1c20-458f-9a9c-0be1b3d417b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1337199b-e0f2-4b34-b1de-f39c2d8458f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bfa0a5a2-6633-492a-97f9-678b5d6889e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"word_freq_make\", \"word_freq_address\",  # Add column names based on the dataset\n",
    "         # ...\n",
    "         \"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0b5a1837-070f-4565-8824-248da90b1d19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['word_freq_make', 'word_freq_address', 'spam']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "76ce02c4-316a-460a-b757-f476edbc7ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(url,names = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1d59aa23-e212-41ec-8e00-6dae49ae94d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.29</th>\n",
       "      <th>1.93</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.96</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>3.756</th>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.21</th>\n",
       "      <th>0.28</th>\n",
       "      <th>0.50</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.14</th>\n",
       "      <th>0.28</th>\n",
       "      <th>0.21</th>\n",
       "      <th>0.07</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.94</th>\n",
       "      <th>0.21</th>\n",
       "      <th>0.79</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.21</th>\n",
       "      <th>0.14</th>\n",
       "      <th>0.14</th>\n",
       "      <th>0.07</th>\n",
       "      <th>0.28</th>\n",
       "      <th>3.47</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.59</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.07</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.132</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.372</th>\n",
       "      <th>0.180</th>\n",
       "      <th>0.048</th>\n",
       "      <th>5.114</th>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.71</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.23</th>\n",
       "      <th>0.19</th>\n",
       "      <th>0.19</th>\n",
       "      <th>0.12</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.25</th>\n",
       "      <th>0.38</th>\n",
       "      <th>0.45</th>\n",
       "      <th>0.12</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.75</th>\n",
       "      <th>0.06</th>\n",
       "      <th>0.06</th>\n",
       "      <th>1.03</th>\n",
       "      <th>1.36</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.51</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.16</th>\n",
       "      <th>0.06</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.06</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.12</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.06</th>\n",
       "      <th>0.06</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.010</th>\n",
       "      <th>0.143</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.276</th>\n",
       "      <th>0.184</th>\n",
       "      <th>0.010</th>\n",
       "      <th>9.821</th>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.63</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.31</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.63</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.31</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.63</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.31</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.31</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.31</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.31</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">3.18</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.31</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.000</th>\n",
       "      <th>0.137</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.137</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>3.537</th>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.135</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.135</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>3.537</th>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.31</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.62</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.31</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.88</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.62</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.31</th>\n",
       "      <th>0.31</th>\n",
       "      <th>0.31</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.232</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>1.142</th>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>6.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>2.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>2.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.353</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>1.555</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.30</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.80</th>\n",
       "      <th>0.30</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.90</th>\n",
       "      <th>1.50</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.30</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.20</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.102</th>\n",
       "      <th>0.718</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>1.404</th>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.96</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.93</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.057</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>1.147</th>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>4.60</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.97</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.125</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>1.250</th>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                     word_freq_make  \\\n",
       "0.00 0.64 0.64 0.0 0.32 0.00 0.00 0.00 0.00 0.00 0.00 0.64 0.00 0.00 0.00 0.32 0.00 1.29 1.93 0.00 0.96 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.000 0.0 0.778 0.000 0.000 3.756              61   \n",
       "0.21 0.28 0.50 0.0 0.14 0.28 0.21 0.07 0.00 0.94 0.21 0.79 0.65 0.21 0.14 0.14 0.07 0.28 3.47 0.00 1.59 0.0 0.43 0.43 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.07 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.132 0.0 0.372 0.180 0.048 5.114             101   \n",
       "0.06 0.00 0.71 0.0 1.23 0.19 0.19 0.12 0.64 0.25 0.38 0.45 0.12 0.00 1.75 0.06 0.06 1.03 1.36 0.32 0.51 0.0 1.16 0.06 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.06 0.0 0.0 0.12 0.00 0.06 0.06 0.0 0.0 0.010 0.143 0.0 0.276 0.184 0.010 9.821             485   \n",
       "0.00 0.00 0.00 0.0 0.63 0.00 0.31 0.63 0.31 0.63 0.31 0.31 0.31 0.00 0.00 0.31 0.00 0.00 3.18 0.00 0.31 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.137 0.0 0.137 0.000 0.000 3.537              40   \n",
       "                                                                                                                                                                                                                                  0.135 0.0 0.135 0.000 0.000 3.537              40   \n",
       "...                                                                                                                                                                                                                                                                             ...   \n",
       "0.31 0.00 0.62 0.0 0.00 0.31 0.00 0.00 0.00 0.00 0.00 1.88 0.00 0.00 0.00 0.00 0.00 0.00 0.62 0.00 0.00 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.31 0.31 0.31 0.0 0.0 0.000 0.232 0.0 0.000 0.000 0.000 1.142               3   \n",
       "0.00 0.00 0.00 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6.00 0.00 2.00 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 2.00 0.0 0.0 0.000 0.000 0.0 0.353 0.000 0.000 1.555               4   \n",
       "0.30 0.00 0.30 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.80 0.30 0.00 0.00 0.00 0.00 0.90 1.50 0.00 0.30 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 1.20 0.0 0.0 0.102 0.718 0.0 0.000 0.000 0.000 1.404               6   \n",
       "0.96 0.00 0.00 0.0 0.32 0.00 0.00 0.00 0.00 0.00 0.00 0.32 0.00 0.00 0.00 0.00 0.00 0.00 1.93 0.00 0.32 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.32 0.00 0.32 0.0 0.0 0.000 0.057 0.0 0.000 0.000 0.000 1.147               5   \n",
       "0.00 0.00 0.65 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.65 0.00 0.00 0.00 0.00 0.00 4.60 0.00 0.65 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 1.97 0.65 0.0 0.0 0.000 0.000 0.0 0.125 0.000 0.000 1.250               5   \n",
       "\n",
       "                                                                                                                                                                                                                                                                     word_freq_address  \\\n",
       "0.00 0.64 0.64 0.0 0.32 0.00 0.00 0.00 0.00 0.00 0.00 0.64 0.00 0.00 0.00 0.32 0.00 1.29 1.93 0.00 0.96 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.000 0.0 0.778 0.000 0.000 3.756                278   \n",
       "0.21 0.28 0.50 0.0 0.14 0.28 0.21 0.07 0.00 0.94 0.21 0.79 0.65 0.21 0.14 0.14 0.07 0.28 3.47 0.00 1.59 0.0 0.43 0.43 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.07 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.132 0.0 0.372 0.180 0.048 5.114               1028   \n",
       "0.06 0.00 0.71 0.0 1.23 0.19 0.19 0.12 0.64 0.25 0.38 0.45 0.12 0.00 1.75 0.06 0.06 1.03 1.36 0.32 0.51 0.0 1.16 0.06 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.06 0.0 0.0 0.12 0.00 0.06 0.06 0.0 0.0 0.010 0.143 0.0 0.276 0.184 0.010 9.821               2259   \n",
       "0.00 0.00 0.00 0.0 0.63 0.00 0.31 0.63 0.31 0.63 0.31 0.31 0.31 0.00 0.00 0.31 0.00 0.00 3.18 0.00 0.31 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.137 0.0 0.137 0.000 0.000 3.537                191   \n",
       "                                                                                                                                                                                                                                  0.135 0.0 0.135 0.000 0.000 3.537                191   \n",
       "...                                                                                                                                                                                                                                                                                ...   \n",
       "0.31 0.00 0.62 0.0 0.00 0.31 0.00 0.00 0.00 0.00 0.00 1.88 0.00 0.00 0.00 0.00 0.00 0.00 0.62 0.00 0.00 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.31 0.31 0.31 0.0 0.0 0.000 0.232 0.0 0.000 0.000 0.000 1.142                 88   \n",
       "0.00 0.00 0.00 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6.00 0.00 2.00 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 2.00 0.0 0.0 0.000 0.000 0.0 0.353 0.000 0.000 1.555                 14   \n",
       "0.30 0.00 0.30 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.80 0.30 0.00 0.00 0.00 0.00 0.90 1.50 0.00 0.30 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 1.20 0.0 0.0 0.102 0.718 0.0 0.000 0.000 0.000 1.404                118   \n",
       "0.96 0.00 0.00 0.0 0.32 0.00 0.00 0.00 0.00 0.00 0.00 0.32 0.00 0.00 0.00 0.00 0.00 0.00 1.93 0.00 0.32 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.32 0.00 0.32 0.0 0.0 0.000 0.057 0.0 0.000 0.000 0.000 1.147                 78   \n",
       "0.00 0.00 0.65 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.65 0.00 0.00 0.00 0.00 0.00 4.60 0.00 0.65 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 1.97 0.65 0.0 0.0 0.000 0.000 0.0 0.125 0.000 0.000 1.250                 40   \n",
       "\n",
       "                                                                                                                                                                                                                                                                     spam  \n",
       "0.00 0.64 0.64 0.0 0.32 0.00 0.00 0.00 0.00 0.00 0.00 0.64 0.00 0.00 0.00 0.32 0.00 1.29 1.93 0.00 0.96 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.000 0.0 0.778 0.000 0.000 3.756     1  \n",
       "0.21 0.28 0.50 0.0 0.14 0.28 0.21 0.07 0.00 0.94 0.21 0.79 0.65 0.21 0.14 0.14 0.07 0.28 3.47 0.00 1.59 0.0 0.43 0.43 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.07 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.132 0.0 0.372 0.180 0.048 5.114     1  \n",
       "0.06 0.00 0.71 0.0 1.23 0.19 0.19 0.12 0.64 0.25 0.38 0.45 0.12 0.00 1.75 0.06 0.06 1.03 1.36 0.32 0.51 0.0 1.16 0.06 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.06 0.0 0.0 0.12 0.00 0.06 0.06 0.0 0.0 0.010 0.143 0.0 0.276 0.184 0.010 9.821     1  \n",
       "0.00 0.00 0.00 0.0 0.63 0.00 0.31 0.63 0.31 0.63 0.31 0.31 0.31 0.00 0.00 0.31 0.00 0.00 3.18 0.00 0.31 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.137 0.0 0.137 0.000 0.000 3.537     1  \n",
       "                                                                                                                                                                                                                                  0.135 0.0 0.135 0.000 0.000 3.537     1  \n",
       "...                                                                                                                                                                                                                                                                   ...  \n",
       "0.31 0.00 0.62 0.0 0.00 0.31 0.00 0.00 0.00 0.00 0.00 1.88 0.00 0.00 0.00 0.00 0.00 0.00 0.62 0.00 0.00 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.31 0.31 0.31 0.0 0.0 0.000 0.232 0.0 0.000 0.000 0.000 1.142     0  \n",
       "0.00 0.00 0.00 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6.00 0.00 2.00 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 2.00 0.0 0.0 0.000 0.000 0.0 0.353 0.000 0.000 1.555     0  \n",
       "0.30 0.00 0.30 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.80 0.30 0.00 0.00 0.00 0.00 0.90 1.50 0.00 0.30 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 1.20 0.0 0.0 0.102 0.718 0.0 0.000 0.000 0.000 1.404     0  \n",
       "0.96 0.00 0.00 0.0 0.32 0.00 0.00 0.00 0.00 0.00 0.00 0.32 0.00 0.00 0.00 0.00 0.00 0.00 1.93 0.00 0.32 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.32 0.00 0.32 0.0 0.0 0.000 0.057 0.0 0.000 0.000 0.000 1.147     0  \n",
       "0.00 0.00 0.65 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.65 0.00 0.00 0.00 0.00 0.00 4.60 0.00 0.65 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 1.97 0.65 0.0 0.0 0.000 0.000 0.0 0.125 0.000 0.000 1.250     0  \n",
       "\n",
       "[4601 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b0b703fa-5905-4de9-aceb-04519ea7e369",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependant and independant feature\n",
    "x= data.drop(\"spam\", axis=1)\n",
    "y = data[\"spam\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "180b0ae8-666e-43cb-9be7-bb4ac322d4ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.29</th>\n",
       "      <th>1.93</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.96</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.778</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>3.756</th>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.21</th>\n",
       "      <th>0.28</th>\n",
       "      <th>0.50</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.14</th>\n",
       "      <th>0.28</th>\n",
       "      <th>0.21</th>\n",
       "      <th>0.07</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.94</th>\n",
       "      <th>0.21</th>\n",
       "      <th>0.79</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.21</th>\n",
       "      <th>0.14</th>\n",
       "      <th>0.14</th>\n",
       "      <th>0.07</th>\n",
       "      <th>0.28</th>\n",
       "      <th>3.47</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.59</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.43</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.07</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.132</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.372</th>\n",
       "      <th>0.180</th>\n",
       "      <th>0.048</th>\n",
       "      <th>5.114</th>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.06</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.71</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.23</th>\n",
       "      <th>0.19</th>\n",
       "      <th>0.19</th>\n",
       "      <th>0.12</th>\n",
       "      <th>0.64</th>\n",
       "      <th>0.25</th>\n",
       "      <th>0.38</th>\n",
       "      <th>0.45</th>\n",
       "      <th>0.12</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.75</th>\n",
       "      <th>0.06</th>\n",
       "      <th>0.06</th>\n",
       "      <th>1.03</th>\n",
       "      <th>1.36</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.51</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.16</th>\n",
       "      <th>0.06</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.06</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.12</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.06</th>\n",
       "      <th>0.06</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.010</th>\n",
       "      <th>0.143</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.276</th>\n",
       "      <th>0.184</th>\n",
       "      <th>0.010</th>\n",
       "      <th>9.821</th>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.63</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.31</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.63</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.31</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.63</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.31</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.31</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.31</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.31</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">3.18</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.31</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.00</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.0</th>\n",
       "      <th rowspan=\"2\" valign=\"top\">0.000</th>\n",
       "      <th>0.137</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.137</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>3.537</th>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.135</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.135</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>3.537</th>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.31</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.62</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.31</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.88</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.62</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.31</th>\n",
       "      <th>0.31</th>\n",
       "      <th>0.31</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.232</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>1.142</th>\n",
       "      <td>3</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>6.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>2.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>2.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.353</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>1.555</th>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.30</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.30</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.80</th>\n",
       "      <th>0.30</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.90</th>\n",
       "      <th>1.50</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.30</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.20</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.102</th>\n",
       "      <th>0.718</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>1.404</th>\n",
       "      <td>6</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.96</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.93</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.32</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.057</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>1.147</th>\n",
       "      <td>5</td>\n",
       "      <td>78</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>4.60</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.00</th>\n",
       "      <th>0.00</th>\n",
       "      <th>1.97</th>\n",
       "      <th>0.65</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.0</th>\n",
       "      <th>0.125</th>\n",
       "      <th>0.000</th>\n",
       "      <th>0.000</th>\n",
       "      <th>1.250</th>\n",
       "      <td>5</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4601 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                     word_freq_make  \\\n",
       "0.00 0.64 0.64 0.0 0.32 0.00 0.00 0.00 0.00 0.00 0.00 0.64 0.00 0.00 0.00 0.32 0.00 1.29 1.93 0.00 0.96 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.000 0.0 0.778 0.000 0.000 3.756              61   \n",
       "0.21 0.28 0.50 0.0 0.14 0.28 0.21 0.07 0.00 0.94 0.21 0.79 0.65 0.21 0.14 0.14 0.07 0.28 3.47 0.00 1.59 0.0 0.43 0.43 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.07 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.132 0.0 0.372 0.180 0.048 5.114             101   \n",
       "0.06 0.00 0.71 0.0 1.23 0.19 0.19 0.12 0.64 0.25 0.38 0.45 0.12 0.00 1.75 0.06 0.06 1.03 1.36 0.32 0.51 0.0 1.16 0.06 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.06 0.0 0.0 0.12 0.00 0.06 0.06 0.0 0.0 0.010 0.143 0.0 0.276 0.184 0.010 9.821             485   \n",
       "0.00 0.00 0.00 0.0 0.63 0.00 0.31 0.63 0.31 0.63 0.31 0.31 0.31 0.00 0.00 0.31 0.00 0.00 3.18 0.00 0.31 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.137 0.0 0.137 0.000 0.000 3.537              40   \n",
       "                                                                                                                                                                                                                                  0.135 0.0 0.135 0.000 0.000 3.537              40   \n",
       "...                                                                                                                                                                                                                                                                             ...   \n",
       "0.31 0.00 0.62 0.0 0.00 0.31 0.00 0.00 0.00 0.00 0.00 1.88 0.00 0.00 0.00 0.00 0.00 0.00 0.62 0.00 0.00 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.31 0.31 0.31 0.0 0.0 0.000 0.232 0.0 0.000 0.000 0.000 1.142               3   \n",
       "0.00 0.00 0.00 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6.00 0.00 2.00 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 2.00 0.0 0.0 0.000 0.000 0.0 0.353 0.000 0.000 1.555               4   \n",
       "0.30 0.00 0.30 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.80 0.30 0.00 0.00 0.00 0.00 0.90 1.50 0.00 0.30 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 1.20 0.0 0.0 0.102 0.718 0.0 0.000 0.000 0.000 1.404               6   \n",
       "0.96 0.00 0.00 0.0 0.32 0.00 0.00 0.00 0.00 0.00 0.00 0.32 0.00 0.00 0.00 0.00 0.00 0.00 1.93 0.00 0.32 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.32 0.00 0.32 0.0 0.0 0.000 0.057 0.0 0.000 0.000 0.000 1.147               5   \n",
       "0.00 0.00 0.65 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.65 0.00 0.00 0.00 0.00 0.00 4.60 0.00 0.65 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 1.97 0.65 0.0 0.0 0.000 0.000 0.0 0.125 0.000 0.000 1.250               5   \n",
       "\n",
       "                                                                                                                                                                                                                                                                     word_freq_address  \n",
       "0.00 0.64 0.64 0.0 0.32 0.00 0.00 0.00 0.00 0.00 0.00 0.64 0.00 0.00 0.00 0.32 0.00 1.29 1.93 0.00 0.96 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.000 0.0 0.778 0.000 0.000 3.756                278  \n",
       "0.21 0.28 0.50 0.0 0.14 0.28 0.21 0.07 0.00 0.94 0.21 0.79 0.65 0.21 0.14 0.14 0.07 0.28 3.47 0.00 1.59 0.0 0.43 0.43 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.07 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.132 0.0 0.372 0.180 0.048 5.114               1028  \n",
       "0.06 0.00 0.71 0.0 1.23 0.19 0.19 0.12 0.64 0.25 0.38 0.45 0.12 0.00 1.75 0.06 0.06 1.03 1.36 0.32 0.51 0.0 1.16 0.06 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.06 0.0 0.0 0.12 0.00 0.06 0.06 0.0 0.0 0.010 0.143 0.0 0.276 0.184 0.010 9.821               2259  \n",
       "0.00 0.00 0.00 0.0 0.63 0.00 0.31 0.63 0.31 0.63 0.31 0.31 0.31 0.00 0.00 0.31 0.00 0.00 3.18 0.00 0.31 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 0.00 0.0 0.0 0.000 0.137 0.0 0.137 0.000 0.000 3.537                191  \n",
       "                                                                                                                                                                                                                                  0.135 0.0 0.135 0.000 0.000 3.537                191  \n",
       "...                                                                                                                                                                                                                                                                                ...  \n",
       "0.31 0.00 0.62 0.0 0.00 0.31 0.00 0.00 0.00 0.00 0.00 1.88 0.00 0.00 0.00 0.00 0.00 0.00 0.62 0.00 0.00 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.31 0.31 0.31 0.0 0.0 0.000 0.232 0.0 0.000 0.000 0.000 1.142                 88  \n",
       "0.00 0.00 0.00 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 6.00 0.00 2.00 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 2.00 0.0 0.0 0.000 0.000 0.0 0.353 0.000 0.000 1.555                 14  \n",
       "0.30 0.00 0.30 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 1.80 0.30 0.00 0.00 0.00 0.00 0.90 1.50 0.00 0.30 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 0.00 1.20 0.0 0.0 0.102 0.718 0.0 0.000 0.000 0.000 1.404                118  \n",
       "0.96 0.00 0.00 0.0 0.32 0.00 0.00 0.00 0.00 0.00 0.00 0.32 0.00 0.00 0.00 0.00 0.00 0.00 1.93 0.00 0.32 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.32 0.00 0.32 0.0 0.0 0.000 0.057 0.0 0.000 0.000 0.000 1.147                 78  \n",
       "0.00 0.00 0.65 0.0 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.65 0.00 0.00 0.00 0.00 0.00 4.60 0.00 0.65 0.0 0.00 0.00 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.00 0.0 0.0 0.00 0.0 0.0 0.00 0.00 1.97 0.65 0.0 0.0 0.000 0.000 0.0 0.125 0.000 0.000 1.250                 40  \n",
       "\n",
       "[4601 rows x 2 columns]"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ddd36cd1-51a0-47e4-80be-2e884a8cfe05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  0.00  0.64  0.00  0.00  0.00  0.32  0.00  1.29  1.93  0.00  0.96  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0  0.00  0.0  0.0  0.00  0.00  0.00  0.00  0.0  0.0  0.000  0.000  0.0  0.778  0.000  0.000  3.756    1\n",
       "0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  0.21  0.79  0.65  0.21  0.14  0.14  0.07  0.28  3.47  0.00  1.59  0.0  0.43  0.43  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.07  0.0  0.0  0.00  0.0  0.0  0.00  0.00  0.00  0.00  0.0  0.0  0.000  0.132  0.0  0.372  0.180  0.048  5.114    1\n",
       "0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  0.38  0.45  0.12  0.00  1.75  0.06  0.06  1.03  1.36  0.32  0.51  0.0  1.16  0.06  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0  0.06  0.0  0.0  0.12  0.00  0.06  0.06  0.0  0.0  0.010  0.143  0.0  0.276  0.184  0.010  9.821    1\n",
       "0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  0.31  0.31  0.31  0.00  0.00  0.31  0.00  0.00  3.18  0.00  0.31  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0  0.00  0.0  0.0  0.00  0.00  0.00  0.00  0.0  0.0  0.000  0.137  0.0  0.137  0.000  0.000  3.537    1\n",
       "                                                                                                                                                                                                                                                                                   0.135  0.0  0.135  0.000  0.000  3.537    1\n",
       "                                                                                                                                                                                                                                                                                                                            ..\n",
       "0.31  0.00  0.62  0.0  0.00  0.31  0.00  0.00  0.00  0.00  0.00  1.88  0.00  0.00  0.00  0.00  0.00  0.00  0.62  0.00  0.00  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0  0.00  0.0  0.0  0.00  0.31  0.31  0.31  0.0  0.0  0.000  0.232  0.0  0.000  0.000  0.000  1.142    0\n",
       "0.00  0.00  0.00  0.0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  6.00  0.00  2.00  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0  0.00  0.0  0.0  0.00  0.00  0.00  2.00  0.0  0.0  0.000  0.000  0.0  0.353  0.000  0.000  1.555    0\n",
       "0.30  0.00  0.30  0.0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  1.80  0.30  0.00  0.00  0.00  0.00  0.90  1.50  0.00  0.30  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0  0.00  0.0  0.0  0.00  0.00  0.00  1.20  0.0  0.0  0.102  0.718  0.0  0.000  0.000  0.000  1.404    0\n",
       "0.96  0.00  0.00  0.0  0.32  0.00  0.00  0.00  0.00  0.00  0.00  0.32  0.00  0.00  0.00  0.00  0.00  0.00  1.93  0.00  0.32  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0  0.00  0.0  0.0  0.00  0.32  0.00  0.32  0.0  0.0  0.000  0.057  0.0  0.000  0.000  0.000  1.147    0\n",
       "0.00  0.00  0.65  0.0  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.00  0.65  0.00  0.00  0.00  0.00  0.00  4.60  0.00  0.65  0.0  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.00  0.0  0.0  0.00  0.0  0.0  0.00  0.00  1.97  0.65  0.0  0.0  0.000  0.000  0.0  0.125  0.000  0.000  1.250    0\n",
       "Name: spam, Length: 4601, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cb937cbd-3c2b-4646-8210-58aa55e8fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB, MultinomialNB, GaussianNB\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "0fae0e0e-362d-4853-acf2-1fcc804bdec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Bernoulli Naive Bayes\n",
    "bernoulli_clf = BernoulliNB()\n",
    "bernoulli_scores = cross_val_score(bernoulli_clf, x, y, cv=10, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "bdff2820-9d29-42a4-9eaa-52db28926ce7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulli_clf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "142f0a75-5124-4b82-8622-d299bfa113a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.60520607, 0.60434783, 0.60434783, 0.60652174, 0.60652174,\n",
       "       0.60652174, 0.60652174, 0.60652174, 0.60652174, 0.60652174])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulli_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "f8acf0a7-1bdc-46ce-895c-265f3e71e610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>BernoulliNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">BernoulliNB</label><div class=\"sk-toggleable__content\"><pre>BernoulliNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bernoulli_clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8835b364-46f4-4c9b-82d2-8e8cb4d5ca46",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = bernoulli_clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "480327d5-c55e-433e-9e89-8cb751d7f9c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ee76f923-f4dc-49eb-a3a4-182529c5a08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0e5a9bbc-341d-4e23-964b-32dc7d2c4129",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/opt/conda/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "confusion_matrix = confusion_matrix(y,y_pred)\n",
    "accuracy_score = accuracy_score(y,y_pred)\n",
    "classification_report = classification_report(y,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "cdf9bbc0-3662-47a3-9b1f-a6f9d106fd40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2788,    0],\n",
       "       [1813,    0]])"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "8dcfc332-c68d-4602-a081-2734a66f308b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6059552271245382"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "34e6f10b-deef-4e8f-b1dc-1bd1a46aaa80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.61      1.00      0.75      2788\\n           1       0.00      0.00      0.00      1813\\n\\n    accuracy                           0.61      4601\\n   macro avg       0.30      0.50      0.38      4601\\nweighted avg       0.37      0.61      0.46      4601\\n'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a0c95a-1251-4546-b4af-56631df95521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Multinomial Naive Bayes\n",
    "multinomial_clf = MultinomialNB()\n",
    "multinomial_scores = cross_val_score(multinomial_clf, x, y, cv=10, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9c6f7956-4552-4058-be65-53bc2e2004cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ea5dd864-3de9-4901-9088-b4467f7866f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.57049892, 0.55217391, 0.5326087 , 0.61521739, 0.48695652,\n",
       "       0.56956522, 0.65434783, 0.59130435, 0.60434783, 0.53695652])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "842ad0e3-1a73-4bf3-a8e4-c3b15c2a4746",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MultinomialNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MultinomialNB</label><div class=\"sk-toggleable__content\"><pre>MultinomialNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MultinomialNB()"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "multinomial_clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "de6cd799-8ab3-40d5-965d-5dbbe4d099eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = multinomial_clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7c479fe5-f500-4933-a648-1982c0ec1086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6bdd5751-24cb-48cc-9ef0-f512cd7472d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "6f0f0254-2852-4b98-bc17-343cb4a25079",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(y,y_pred1)\n",
    "accuracy_score = accuracy_score(y,y_pred1)\n",
    "classification_report = classification_report(y,y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "32398916-f4f3-4a2c-a026-98338715aeb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1623, 1165],\n",
       "       [ 807, 1006]])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "1feed86b-5c47-4e94-badc-cb1bc8c91198",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5713975222777657"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "8c0dc710-fe92-4ec0-b030-bcd29d717b69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.67      0.58      0.62      2788\\n           1       0.46      0.55      0.51      1813\\n\\n    accuracy                           0.57      4601\\n   macro avg       0.57      0.57      0.56      4601\\nweighted avg       0.59      0.57      0.58      4601\\n'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3f787888-1c94-41ed-bf9e-2176bb96fa4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement Gaussian Naive Bayes\n",
    "gaussian_clf = GaussianNB()\n",
    "gaussian_scores = cross_val_score(gaussian_clf, X, y, cv=10, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4a90b9c7-05ee-4d59-ae37-e64c482e666f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" checked><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_clf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "85fdcbb8-4255-4510-8cab-e737e3375cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.64859002, 0.64782609, 0.67173913, 0.69782609, 0.66304348,\n",
       "       0.67608696, 0.68695652, 0.67391304, 0.69782609, 0.68913043])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_scores "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "bab74823-9896-49de-9d54-ee9be89376c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianNB()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianNB</label><div class=\"sk-toggleable__content\"><pre>GaussianNB()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianNB()"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gaussian_clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "d16e648b-dd35-43ca-a705-50e83dab90f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = gaussian_clf.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "477168af-9a4c-4af5-a716-21576ca93a38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "e9f0d31f-1683-4713-9df9-6b5c3b48eb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,accuracy_score,classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "5a4338d4-d0af-4a9e-815d-e160c69b27d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = confusion_matrix(y,y_pred2)\n",
    "accuracy_score = accuracy_score(y,y_pred2)\n",
    "classification_report = classification_report(y,y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "f2d196c2-6235-44f9-8972-8d7aad72f3f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2682,  106],\n",
       "       [1392,  421]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "94f28863-eb6d-48ac-ac16-16683a7d9c1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6744186046511628"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "94db3833-234b-4c68-992e-9f353eae3eff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.66      0.96      0.78      2788\\n           1       0.80      0.23      0.36      1813\\n\\n    accuracy                           0.67      4601\\n   macro avg       0.73      0.60      0.57      4601\\nweighted avg       0.71      0.67      0.62      4601\\n'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "bd2530d5-9017-4092-b13d-4931d19f4be2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Accuracy: 0.6059553899839669\n",
      "Multinomial Naive Bayes Accuracy: 0.5713977176270867\n",
      "Gaussian Naive Bayes Accuracy: 0.675293784777893\n"
     ]
    }
   ],
   "source": [
    "print(\"Bernoulli Naive Bayes Accuracy:\", bernoulli_scores.mean())\n",
    "print(\"Multinomial Naive Bayes Accuracy:\", multinomial_scores.mean())\n",
    "print(\"Gaussian Naive Bayes Accuracy:\", gaussian_scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b45a742-db69-4d77-8a38-bf826f6da9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "Inference:\n",
    "    Gaussian naive bayes algorithm has high accuracy compared to othe naive bayes algorithm.so, the Gaussian naive bayes algorithm performed best.\n",
    "    Some limitations of the Naive Bayes algorithm include its strong assumption of feature independence, which might not hold true for certain datasets. \n",
    "    Additionally, Naive Bayes tends to struggle with datasets where the underlying distribution is not well represented by the chosen distribution (e.g., Gaussian for continuous features). \n",
    "    It can also be sensitive to the presence of irrelevant or correlated features, which can affect its overall performance.  \n",
    "    To improve the performance, one could explore more advanced feature engineering techniques, such as selecting more relevant features, handling missing values effectively, and possibly using more sophisticated models like decision trees, random forests, or ensemble methods, especially for datasets where the feature independence assumption might not hold. \n",
    "    Regularization techniques could also be employed to mitigate overfitting issues. \n",
    "    Additionally, using more extensive datasets for training and exploring other feature extraction methods could potentially enhance the performance of the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4215bac-d180-4a9e-b685-e5356350ccbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafd1a9f-5fb6-45c8-bc80-972449a9c1ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
